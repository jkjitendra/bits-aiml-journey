{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q-LEARNING ON FROZEN LAKE (Deterministic Environment)**"
      ],
      "metadata": {
        "id": "i5tCusRU1AkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium numpy matplotlib --quiet\n"
      ],
      "metadata": {
        "id": "cY9SEXLmoMxg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "IRO1TqKs1Dmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "metadata": {
        "id": "Ts6ZK34U1ICi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Environment"
      ],
      "metadata": {
        "id": "fRZx4yoE1J7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FrozenLake-v1 has a 4x4 grid world:\n",
        "# S - Start, F - Frozen (safe), H - Hole (danger), G - Goal\n",
        "# Setting is_slippery=False makes it deterministic (no random slips)\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
      ],
      "metadata": {
        "id": "XZh04dPl1NTH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize State & Action Space"
      ],
      "metadata": {
        "id": "Ipbei7Ak1PXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n       # Number of possible actions (4)\n",
        "state_space_size = env.observation_space.n   # Number of possible states (16)"
      ],
      "metadata": {
        "id": "nCfSsEXI1Rqd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Q-Table"
      ],
      "metadata": {
        "id": "gk_46nOs1UmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Q-table stores the value of each state‚Äìaction pair\n",
        "# Rows = states, Columns = actions\n",
        "qtable = np.zeros((state_space_size, action_space_size))"
      ],
      "metadata": {
        "id": "e-Xb8j_E1Wrs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Hyperparameters"
      ],
      "metadata": {
        "id": "VVhHqqb71ZdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_episodes = 5000     # Total number of training episodes\n",
        "max_steps = 100           # Max steps per episode\n",
        "\n",
        "learning_rate = 0.8       # Alpha ‚Äì learning rate\n",
        "gamma = 0.95              # Gamma ‚Äì discount factor"
      ],
      "metadata": {
        "id": "JYeNfh9z1bS6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploration Parameters"
      ],
      "metadata": {
        "id": "5s91NFTI1dOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1.0             # Exploration rate\n",
        "max_epsilon = 1.0         # Max exploration rate\n",
        "min_epsilon = 0.01        # Min exploration rate\n",
        "decay_rate = 0.0005       # Exponential decay rate for epsilon"
      ],
      "metadata": {
        "id": "a9FVPJGJ1fi8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Preparation"
      ],
      "metadata": {
        "id": "COPLjavS1hig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []              # Track rewards per episode"
      ],
      "metadata": {
        "id": "55mUGf7N1iEG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-LEARNING TRAINING LOOP"
      ],
      "metadata": {
        "id": "e5T5o5Yt1lzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not hasattr(np, \"bool8\"):  # Compatibility for NumPy 2.0+\n",
        "    np.bool8 = np.bool_\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment at the start of each episode\n",
        "    state_return = env.reset()\n",
        "    state = state_return[0] if isinstance(state_return, tuple) else state_return\n",
        "\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # -----------------------------\n",
        "        # üîπ Exploration‚ÄìExploitation Trade-off\n",
        "        # -----------------------------\n",
        "        if random.uniform(0, 1) > epsilon:\n",
        "            # Exploitation: choose best action from Q-table\n",
        "            action = np.argmax(qtable[state, :])\n",
        "        else:\n",
        "            # Exploration: choose a random action\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # -----------------------------\n",
        "        # üîπ Perform Action & Observe Result\n",
        "        # -----------------------------\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 5:\n",
        "            new_state, reward, done, truncated, info = step_result\n",
        "        else:\n",
        "            new_state, reward, done, info = step_result\n",
        "            truncated = False\n",
        "\n",
        "        # -----------------------------\n",
        "        # üîπ Reward Shaping\n",
        "        # -----------------------------\n",
        "        # Give higher reward for reaching goal to encourage success\n",
        "        if reward == 1:\n",
        "            reward = 10\n",
        "\n",
        "        # -----------------------------\n",
        "        # üîπ Update Q-Table (Bellman Equation)\n",
        "        # -----------------------------\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (\n",
        "            reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action]\n",
        "        )\n",
        "\n",
        "        total_rewards += reward\n",
        "        state = new_state\n",
        "\n",
        "        # Stop episode if goal or hole reached\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    # -----------------------------\n",
        "    # üîπ Epsilon Decay (reduces exploration over time)\n",
        "    # -----------------------------\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "    # Store total rewards for analysis\n",
        "    rewards.append(total_rewards)"
      ],
      "metadata": {
        "id": "8zBOhEqD1rPr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING RESULTS"
      ],
      "metadata": {
        "id": "F2jt4NMf14O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "average_score = sum(rewards) / total_episodes\n",
        "print(\"===============================================================\")\n",
        "print(f\"Training Complete! Average Score over time: {average_score:.5f}\")\n",
        "print(\"===============================================================\")\n",
        "print(\"Final Q-Table (State-Action Values):\")\n",
        "print(np.array2string(qtable, formatter={'float_kind': lambda x: f'{x:.8f}'}))\n",
        "print(\"===============================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgpKsxcY08TP",
        "outputId": "c7e8fed5-e852-43f7-8441-60ebd66179a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================\n",
            "Training Complete! Average Score over time: 0.00000\n",
            "===============================================================\n",
            "Final Q-Table (State-Action Values):\n",
            "[[7.35091891 7.73780937 7.73780937 7.35091891]\n",
            " [7.35091891 0.00000000 8.14506250 7.73780937]\n",
            " [7.73780937 8.57375000 7.73780937 8.14506250]\n",
            " [8.14506250 0.00000000 7.73780937 7.73780937]\n",
            " [7.73780937 8.14506250 0.00000000 7.35091891]\n",
            " [0.00000000 0.00000000 0.00000000 0.00000000]\n",
            " [0.00000000 9.02500000 0.00000000 8.14506250]\n",
            " [0.00000000 0.00000000 0.00000000 0.00000000]\n",
            " [8.14506250 0.00000000 8.57375000 7.73780937]\n",
            " [8.14506250 9.02500000 9.02500000 0.00000000]\n",
            " [8.57375000 9.50000000 0.00000000 8.57375000]\n",
            " [0.00000000 0.00000000 0.00000000 0.00000000]\n",
            " [0.00000000 0.00000000 0.00000000 0.00000000]\n",
            " [0.00000000 9.02500000 9.50000000 8.57375000]\n",
            " [9.02500000 9.50000000 10.00000000 9.02500000]\n",
            " [0.00000000 0.00000000 0.00000000 0.00000000]]\n",
            "===============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nüéÆ Testing the trained agent...\\n\")\n",
        "\n",
        "for episode in range(5):\n",
        "    state_return = env.reset()\n",
        "    if isinstance(state_return, tuple):\n",
        "        state = state_return[0]\n",
        "    else:\n",
        "        state = state_return\n",
        "\n",
        "    print(\"****************************************************\")\n",
        "    print(f\"EPISODE {episode + 1}\")\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Choose best action from Q-table\n",
        "        action = np.argmax(qtable[state, :])\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Uncomment below line to render (optional visualization)\n",
        "        # env.render()\n",
        "\n",
        "        if done:\n",
        "            if new_state == 15:\n",
        "                print(\"üèÜ We reached our Goal!\")\n",
        "            else:\n",
        "                print(\"‚ò†Ô∏è We fell into a hole!\")\n",
        "            print(\"Number of steps:\", step)\n",
        "            break\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YWYG92f3KyS",
        "outputId": "f8e59408-cb08-489b-c9d4-2ccc66e7ed47"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéÆ Testing the trained agent...\n",
            "\n",
            "****************************************************\n",
            "EPISODE 1\n",
            "üèÜ We reached our Goal!\n",
            "Number of steps: 5\n",
            "****************************************************\n",
            "EPISODE 2\n",
            "üèÜ We reached our Goal!\n",
            "Number of steps: 5\n",
            "****************************************************\n",
            "EPISODE 3\n",
            "üèÜ We reached our Goal!\n",
            "Number of steps: 5\n",
            "****************************************************\n",
            "EPISODE 4\n",
            "üèÜ We reached our Goal!\n",
            "Number of steps: 5\n",
            "****************************************************\n",
            "EPISODE 5\n",
            "üèÜ We reached our Goal!\n",
            "Number of steps: 5\n"
          ]
        }
      ]
    }
  ]
}